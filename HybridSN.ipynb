{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HybridSN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM7VBIQ8oBiH",
        "colab_type": "text"
      },
      "source": [
        "# HybridSN 高光谱分类  \n",
        "首先取得数据，并引入基本函数库。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koFbItdTn5kv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "ab6f665b-19d9-4fc2-dbf2-a019112a815c"
      },
      "source": [
        "! wget http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\n",
        "! wget http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\n",
        "! pip install spectral"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-11 14:36:46--  http://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\n",
            "Resolving www.ehu.eus (www.ehu.eus)... 158.227.0.65, 2001:720:1410::65\n",
            "Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5953527 (5.7M) [text/plain]\n",
            "Saving to: ‘Indian_pines_corrected.mat’\n",
            "\n",
            "Indian_pines_correc 100%[===================>]   5.68M  1.64MB/s    in 3.8s    \n",
            "\n",
            "2020-08-11 14:36:50 (1.49 MB/s) - ‘Indian_pines_corrected.mat’ saved [5953527/5953527]\n",
            "\n",
            "--2020-08-11 14:36:54--  http://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\n",
            "Resolving www.ehu.eus (www.ehu.eus)... 158.227.0.65, 2001:720:1410::65\n",
            "Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1125 (1.1K) [text/plain]\n",
            "Saving to: ‘Indian_pines_gt.mat’\n",
            "\n",
            "Indian_pines_gt.mat 100%[===================>]   1.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-08-11 14:36:54 (197 MB/s) - ‘Indian_pines_gt.mat’ saved [1125/1125]\n",
            "\n",
            "Collecting spectral\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/06/6a89035cde4eac3ed94e1888f850af653386e8ee827edc72ffc8e445bcb7/spectral-0.22.1-py3-none-any.whl (212kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from spectral) (1.18.5)\n",
            "Installing collected packages: spectral\n",
            "Successfully installed spectral-0.22.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVouTRiMoKGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "import spectral\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKHRU3fIoNfz",
        "colab_type": "text"
      },
      "source": [
        "## 定义 HybridSN 类  \n",
        "![alt text](https://camo.githubusercontent.com/f7db2d6ed93212a9b4d31c2826e38a2bdd964311/68747470733a2f2f67616f707572737569742e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f3230323030332f32303230303330343230333731302e6a7067)  \n",
        "\n",
        "三维卷积部分：\n",
        "\n",
        "conv1：（1, 30, 25, 25）， 8个 7x3x3 的卷积核 ==>  （8, 24, 23, 23）  \n",
        "conv2：（8, 24, 23, 23）， 16个 5x3x3 的卷积核 ==>（16, 20, 21, 21）  \n",
        "conv3：（16, 20, 21, 21），32个 3x3x3 的卷积核 ==>（32, 18, 19, 19）  \n",
        "接下来要进行二维卷积，因此把前面的 32*18 reshape 一下，得到 （576, 19, 19）  \n",
        "\n",
        "二维卷积：（576, 19, 19） 64个 3x3 的卷积核，得到 （64, 17, 17）\n",
        "\n",
        "接下来是一个 flatten 操作，变为 18496 维的向量，\n",
        "\n",
        "接下来依次为256，128节点的全连接层，都使用比例为0.4的 Dropout，\n",
        "\n",
        "最后输出为 16 个节点，是最终的分类类别数。\n",
        "\n",
        "下面是 HybridSN 类的代码："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNa1pSjCoYyj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "f4750ac3-b086-4403-9654-798c22e71c70"
      },
      "source": [
        "# 原始代码\n",
        "class_num = 16\n",
        "\n",
        "class HybridSN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(HybridSN, self).__init__()\n",
        "\t# 3个3D卷积\n",
        "    # conv1：（1, 30, 25, 25）， 8个 7x3x3 的卷积核 ==> （8, 24, 23, 23）\n",
        "    self.conv1_3d = nn.Sequential(\n",
        "        nn.Conv3d(1,8,(7,3,3)),\n",
        "        nn.BatchNorm3d(8),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\t# conv2：（8, 24, 23, 23）， 16个 5x3x3 的卷积核 ==>（16, 20, 21, 21）\n",
        "    self.conv2_3d = nn.Sequential(\n",
        "        nn.Conv3d(8,16,(5,3,3)),\n",
        "        nn.BatchNorm3d(16),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\t# conv3：（16, 20, 21, 21），32个 3x3x3 的卷积核 ==>（32, 18, 19, 19）\n",
        "    self.conv3_3d = nn.Sequential(\n",
        "        nn.Conv3d(16,32,(3,3,3)),\n",
        "        nn.BatchNorm3d(32),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\t# 二维卷积：（576, 19, 19） 64个 3x3 的卷积核，得到 （64, 17, 17）\n",
        "    self.conv4_2d = nn.Sequential(\n",
        "        nn.Conv2d(576,64,(3,3)),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\t# 接下来依次为256，128节点的全连接层，都使用比例为0.1的 Dropout\n",
        "    self.fn1 = nn.Linear(18496,256)\n",
        "    self.fn2 = nn.Linear(256,128)\n",
        "\n",
        "    self.fn_out = nn.Linear(128,class_num)\n",
        "\n",
        "    self.drop = nn.Dropout(p = 0.1)\n",
        "    # emm我在这里使用了softmax之后，网络在训练过程中loss就不再下降了，不知道具体是为啥，很奇怪，，\n",
        "    # self.soft = nn.Softmax(dim = 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1_3d(x)\n",
        "    out = self.conv2_3d(out)\n",
        "    out = self.conv3_3d(out)\n",
        "\t# 进行二维卷积，因此把前面的 32*18 reshape 一下，得到 （576, 19, 19）\n",
        "    b,x,y,m,n = out.size()\n",
        "    out = out.view(b,x*y,m,n)\n",
        "\n",
        "    out = self.conv4_2d(out)\n",
        "\t# 接下来是一个 flatten 操作，变为 18496 维的向量\n",
        "    # 进行重组，以b行，d列的形式存放（d自动计算）\n",
        "    out = out.reshape(b,-1)\n",
        "\n",
        "    out = self.fn1(out)\n",
        "    out = self.drop(out)\n",
        "    out = self.fn2(out)\n",
        "    out = self.drop(out)\n",
        "\n",
        "    out = self.fn_out(out)\n",
        "\n",
        "    # out = self.soft(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "# 随机输入，测试网络结构是否通\n",
        "x = torch.randn(1, 1, 30, 25, 25)\n",
        "net = HybridSN()\n",
        "y = net(x)\n",
        "print(y.shape)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 16])\n",
            "tensor([[ 0.1285,  0.3525, -0.0756,  0.0151, -0.0222,  0.0053,  0.1639,  0.2420,\n",
            "          0.0098, -0.0929, -0.1854, -0.0890,  0.0499,  0.1919,  0.1547, -0.1718]],\n",
            "       grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPU8fGJHEQm9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "cb537cb7-586d-4e9b-8f3e-7979cc291e56"
      },
      "source": [
        "# 模型改进——先使用二位卷积，在使用三位卷积\n",
        "class_num = 16\n",
        "\n",
        "class HybridSN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(HybridSN, self).__init__()\n",
        "\t# 二维卷积：原始输入（30, 25, 25） 64个 3x3x30 的卷积核，得到 （64, 23, 23）\n",
        "    self.conv4_2d = nn.Sequential(\n",
        "        nn.Conv2d(30,64,(3,3)),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    # 三个三维卷积\n",
        "    # conv1：（1, 64, 23, 23）， 8个 7x3x3 的卷积核 ==> （8, 58, 21, 21）\n",
        "    self.conv1_3d = nn.Sequential(\n",
        "        nn.Conv3d(1,8,(7,3,3)),\n",
        "        nn.BatchNorm3d(8),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    # conv2：（8, 58, 21, 21）， 16个 5x3x3 的卷积核 ==> （16, 54, 19, 19）\n",
        "    self.conv2_3d = nn.Sequential(\n",
        "        nn.Conv3d(8,16,(5,3,3)),\n",
        "        nn.BatchNorm3d(16),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    # conv3：（16, 54, 19, 19）， 32个 5x3x3 的卷积核 ==> （32, 52, 17, 17）\n",
        "    self.conv3_3d = nn.Sequential(\n",
        "        nn.Conv3d(16,32,(3,3,3)),\n",
        "        nn.BatchNorm3d(32),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.fn1 = nn.Linear(480896,256)# 32*52*17*17\n",
        "    self.fn2 = nn.Linear(256,128)\n",
        "\n",
        "    self.fn_out = nn.Linear(128,class_num)\n",
        "\n",
        "    self.drop = nn.Dropout(p = 0.4)\n",
        "    # emm我在这里使用了softmax之后，网络在训练过程中loss就不再下降了，不知道具体是为啥，很奇怪，，\n",
        "    # self.soft = nn.Softmax(dim = 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # 先降到二维\n",
        "    out = x.view(x.shape[0],x.shape[2],x.shape[3],x.shape[4])\n",
        "    out = self.conv4_2d(out)\n",
        "    # 增加一个维度（64, 23, 23）-->（1,64, 23, 23）\n",
        "    out = out.view(out.shape[0],1,out.shape[1],out.shape[2],out.shape[3])\n",
        "\n",
        "    out = self.conv1_3d(out)\n",
        "    out = self.conv2_3d(out)\n",
        "    out = self.conv3_3d(out)\n",
        "    # 进行重组，以b行，d列的形式存放（d自动计算）\n",
        "    out = out.view(out.shape[0],-1)\n",
        "\n",
        "    out = self.fn1(out)\n",
        "    out = self.drop(out)\n",
        "    out = self.fn2(out)\n",
        "    out = self.drop(out)\n",
        "\n",
        "    out = self.fn_out(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "# 随机输入，测试网络结构是否通\n",
        "x = torch.randn(1,1, 30, 25, 25)\n",
        "net = HybridSN()\n",
        "y = net(x)\n",
        "print(y.shape)\n",
        "print(y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 16])\n",
            "tensor([[-0.0244, -0.3607, -0.0801,  0.2004, -0.1874, -0.1941, -0.0395,  0.1199,\n",
            "         -0.1129,  0.0493,  0.0176, -0.0850,  0.1632,  0.3491,  0.2695,  0.0121]],\n",
            "       grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPj-zHGdI90d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1fb7bcef-7b8f-4211-e9f6-5fe194cf6e04"
      },
      "source": [
        "# 引入注意力机制\n",
        "class_num = 16\n",
        "\n",
        "\n",
        "class Attention_Block(nn.Module):\n",
        "\n",
        "    def __init__(self, planes, size):\n",
        "        super(Attention_Block, self).__init__()\n",
        "\n",
        "        self.globalAvgPool = nn.AvgPool2d(size, stride=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(planes, round(planes / 16))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(round(planes / 16), planes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.globalAvgPool(x)\n",
        "        out = out.view(out.shape[0], out.shape[1])\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        out = out.view(out.shape[0], out.shape[1], 1, 1)\n",
        "        out = out * residual\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class HybridSN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(HybridSN, self).__init__()\n",
        "        # 3个3D卷积\n",
        "        # conv1：（1, 30, 25, 25）， 8个 7x3x3 的卷积核 ==> （8, 24, 23, 23）\n",
        "        self.conv1_3d = nn.Sequential(\n",
        "            nn.Conv3d(1,8,(7,3,3)),\n",
        "            nn.BatchNorm3d(8),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # conv2：（8, 24, 23, 23）， 16个 5x3x3 的卷积核 ==>（16, 20, 21, 21）\n",
        "        self.conv2_3d = nn.Sequential(\n",
        "            nn.Conv3d(8,16,(5,3,3)),\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # conv3：（16, 20, 21, 21），32个 3x3x3 的卷积核 ==>（32, 18, 19, 19）\n",
        "        self.conv3_3d = nn.Sequential(\n",
        "            nn.Conv3d(16,32,(3,3,3)),\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # 二维卷积：（576, 19, 19） 64个 3x3 的卷积核，得到 （64, 17, 17）\n",
        "        self.conv4_2d = nn.Sequential(\n",
        "            nn.Conv2d(576,64,(3,3)),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # 注意力机制部分\n",
        "        self.layer1 = self.make_layer(Attention_Block,planes = 576, size = 19)\n",
        "        self.layer2 = self.make_layer(Attention_Block,planes = 64, size = 17)\n",
        "\n",
        "        # 接下来依次为256，128节点的全连接层，都使用比例为0.1的 Dropout\n",
        "        self.fn1 = nn.Linear(18496,256)\n",
        "        self.fn2 = nn.Linear(256,128)\n",
        "\n",
        "        self.fn_out = nn.Linear(128,class_num)\n",
        "\n",
        "        self.drop = nn.Dropout(p = 0.1)\n",
        "        # emm我在这里使用了softmax之后，网络在训练过程中loss就不再下降了，不知道具体是为啥，很奇怪，，\n",
        "        # self.soft = nn.Softmax(dim = 1)\n",
        "\n",
        "    def make_layer(self, block, planes, size):\n",
        "        layers = []\n",
        "        layers.append(block(planes, size))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1_3d(x)\n",
        "        out = self.conv2_3d(out)\n",
        "        out = self.conv3_3d(out)\n",
        "        # 进行二维卷积，因此把前面的 32*18 reshape 一下，得到 （576, 19, 19）\n",
        "        out = out.view(out.shape[0],out.shape[1]*out.shape[2],out.shape[3],out.shape[4])\n",
        "\n",
        "        # 在二维卷积部分引入注意力机制\n",
        "        out = self.layer1(out)\n",
        "        out = self.conv4_2d(out)\n",
        "        out = self.layer2(out)\n",
        "        # 接下来是一个 flatten 操作，变为 18496 维的向量\n",
        "        # 进行重组，以b行，d列的形式存放（d自动计算）\n",
        "        out = out.view(out.shape[0],-1)\n",
        "\n",
        "        out = self.fn1(out)\n",
        "        out = self.drop(out)\n",
        "        out = self.fn2(out)\n",
        "        out = self.drop(out)\n",
        "\n",
        "        out = self.fn_out(out)\n",
        "\n",
        "        # out = self.soft(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# 随机输入，测试网络结构是否通\n",
        "x = torch.randn(1, 1, 30, 25, 25)\n",
        "net = HybridSN()\n",
        "y = net(x)\n",
        "print(y.shape)\n",
        "print(y)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 16])\n",
            "tensor([[ 0.2083, -0.0485, -0.0676,  0.1078,  0.0107,  0.0828,  0.0422, -0.0309,\n",
            "          0.0706, -0.0167, -0.1339, -0.0660, -0.1316,  0.0684, -0.1107, -0.0173]],\n",
            "       grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQmE9RAXuv3g",
        "colab_type": "text"
      },
      "source": [
        "## 创建数据集\n",
        "首先对高光谱数据实施PCA降维；然后创建 keras 方便处理的数据格式；然后随机抽取 10% 数据做为训练集，剩余的做为测试集。\n",
        "\n",
        "首先定义基本函数："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXlB3AD3uxu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 对高光谱数据 X 应用 PCA 变换\n",
        "def applyPCA(X, numComponents):\n",
        "    newX = np.reshape(X, (-1, X.shape[2]))\n",
        "    pca = PCA(n_components=numComponents, whiten=True)\n",
        "    newX = pca.fit_transform(newX)\n",
        "    newX = np.reshape(newX, (X.shape[0], X.shape[1], numComponents))\n",
        "    return newX\n",
        "\n",
        "# 对单个像素周围提取 patch 时，边缘像素就无法取了，因此，给这部分像素进行 padding 操作\n",
        "def padWithZeros(X, margin=2):\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX\n",
        "\n",
        "# 在每个像素周围提取 patch ，然后创建成符合 keras 处理的格式\n",
        "def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n",
        "    # 给 X 做 padding\n",
        "    margin = int((windowSize - 1) / 2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    # split patches\n",
        "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels\n",
        "\n",
        "def splitTrainTestSet(X, y, testRatio, randomState=345):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIRRBVkEu3CM",
        "colab_type": "text"
      },
      "source": [
        "下面读取并创建数据集："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQFlh08Wu3d8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "d8756b9d-4f77-4e47-9778-fc1ccddf77c8"
      },
      "source": [
        "# 地物类别\n",
        "class_num = 16\n",
        "X = sio.loadmat('Indian_pines_corrected.mat')['indian_pines_corrected']\n",
        "y = sio.loadmat('Indian_pines_gt.mat')['indian_pines_gt']\n",
        "\n",
        "# 用于测试样本的比例\n",
        "test_ratio = 0.90\n",
        "# 每个像素周围提取 patch 的尺寸\n",
        "patch_size = 25\n",
        "# 使用 PCA 降维，得到主成分的数量\n",
        "pca_components = 30\n",
        "\n",
        "print('Hyperspectral data shape: ', X.shape)\n",
        "print('Label shape: ', y.shape)\n",
        "\n",
        "print('\\n... ... PCA tranformation ... ...')\n",
        "X_pca = applyPCA(X, numComponents=pca_components)\n",
        "print('Data shape after PCA: ', X_pca.shape)\n",
        "\n",
        "print('\\n... ... create data cubes ... ...')\n",
        "X_pca, y = createImageCubes(X_pca, y, windowSize=patch_size)\n",
        "print('Data cube X shape: ', X_pca.shape)\n",
        "print('Data cube y shape: ', y.shape)\n",
        "\n",
        "print('\\n... ... create train & test data ... ...')\n",
        "Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(X_pca, y, test_ratio)\n",
        "print('Xtrain shape: ', Xtrain.shape)\n",
        "print('Xtest  shape: ', Xtest.shape)\n",
        "\n",
        "# 改变 Xtrain, Ytrain 的形状，以符合 keras 的要求\n",
        "Xtrain = Xtrain.reshape(-1, patch_size, patch_size, pca_components, 1)\n",
        "Xtest  = Xtest.reshape(-1, patch_size, patch_size, pca_components, 1)\n",
        "print('before transpose: Xtrain shape: ', Xtrain.shape) \n",
        "print('before transpose: Xtest  shape: ', Xtest.shape) \n",
        "\n",
        "# 为了适应 pytorch 结构，数据要做 transpose\n",
        "Xtrain = Xtrain.transpose(0, 4, 3, 1, 2)\n",
        "Xtest  = Xtest.transpose(0, 4, 3, 1, 2)\n",
        "print('after transpose: Xtrain shape: ', Xtrain.shape) \n",
        "print('after transpose: Xtest  shape: ', Xtest.shape) \n",
        "\n",
        "\n",
        "\"\"\" Training dataset\"\"\"\n",
        "class TrainDS(torch.utils.data.Dataset): \n",
        "    def __init__(self):\n",
        "        self.len = Xtrain.shape[0]\n",
        "        self.x_data = torch.FloatTensor(Xtrain)\n",
        "        self.y_data = torch.LongTensor(ytrain)        \n",
        "    def __getitem__(self, index):\n",
        "        # 根据索引返回数据和对应的标签\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "    def __len__(self): \n",
        "        # 返回文件数据的数目\n",
        "        return self.len\n",
        "\n",
        "\"\"\" Testing dataset\"\"\"\n",
        "class TestDS(torch.utils.data.Dataset): \n",
        "    def __init__(self):\n",
        "        self.len = Xtest.shape[0]\n",
        "        self.x_data = torch.FloatTensor(Xtest)\n",
        "        self.y_data = torch.LongTensor(ytest)\n",
        "    def __getitem__(self, index):\n",
        "        # 根据索引返回数据和对应的标签\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "    def __len__(self): \n",
        "        # 返回文件数据的数目\n",
        "        return self.len\n",
        "\n",
        "# 创建 trainloader 和 testloader\n",
        "trainset = TrainDS()\n",
        "testset  = TestDS()\n",
        "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader  = torch.utils.data.DataLoader(dataset=testset,  batch_size=128, shuffle=False, num_workers=2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyperspectral data shape:  (145, 145, 200)\n",
            "Label shape:  (145, 145)\n",
            "\n",
            "... ... PCA tranformation ... ...\n",
            "Data shape after PCA:  (145, 145, 30)\n",
            "\n",
            "... ... create data cubes ... ...\n",
            "Data cube X shape:  (10249, 25, 25, 30)\n",
            "Data cube y shape:  (10249,)\n",
            "\n",
            "... ... create train & test data ... ...\n",
            "Xtrain shape:  (1024, 25, 25, 30)\n",
            "Xtest  shape:  (9225, 25, 25, 30)\n",
            "before transpose: Xtrain shape:  (1024, 25, 25, 30, 1)\n",
            "before transpose: Xtest  shape:  (9225, 25, 25, 30, 1)\n",
            "after transpose: Xtrain shape:  (1024, 1, 30, 25, 25)\n",
            "after transpose: Xtest  shape:  (9225, 1, 30, 25, 25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_7Wpk-iu8ZM",
        "colab_type": "text"
      },
      "source": [
        "## 开始训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_TYj6UDu-ZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a08962d4-d218-465d-cf93-9a53a83098ec"
      },
      "source": [
        "# 使用GPU训练，可以在菜单 \"代码执行工具\" -> \"更改运行时类型\" 里进行设置\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 网络放到GPU上\n",
        "net = HybridSN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# 开始训练\n",
        "net.train()\n",
        "total_loss = 0\n",
        "for epoch in range(100):\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 优化器梯度归零\n",
        "        optimizer.zero_grad()\n",
        "        # 正向传播 +　反向传播 + 优化 \n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print('[Epoch: %d]   [loss avg: %.4f]   [current loss: %.4f]' %(epoch + 1, total_loss/(epoch+1), loss.item()))\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 1]   [loss avg: 30.1527]   [current loss: 2.3643]\n",
            "[Epoch: 2]   [loss avg: 20.2693]   [current loss: 0.8725]\n",
            "[Epoch: 3]   [loss avg: 15.3539]   [current loss: 0.5051]\n",
            "[Epoch: 4]   [loss avg: 12.2506]   [current loss: 0.2793]\n",
            "[Epoch: 5]   [loss avg: 10.0476]   [current loss: 0.1493]\n",
            "[Epoch: 6]   [loss avg: 8.4768]   [current loss: 0.0752]\n",
            "[Epoch: 7]   [loss avg: 7.3092]   [current loss: 0.0210]\n",
            "[Epoch: 8]   [loss avg: 6.4156]   [current loss: 0.0077]\n",
            "[Epoch: 9]   [loss avg: 5.7171]   [current loss: 0.0052]\n",
            "[Epoch: 10]   [loss avg: 5.1532]   [current loss: 0.0050]\n",
            "[Epoch: 11]   [loss avg: 4.6892]   [current loss: 0.0027]\n",
            "[Epoch: 12]   [loss avg: 4.3027]   [current loss: 0.0020]\n",
            "[Epoch: 13]   [loss avg: 3.9752]   [current loss: 0.0155]\n",
            "[Epoch: 14]   [loss avg: 3.6932]   [current loss: 0.0020]\n",
            "[Epoch: 15]   [loss avg: 3.4484]   [current loss: 0.0042]\n",
            "[Epoch: 16]   [loss avg: 3.2341]   [current loss: 0.0016]\n",
            "[Epoch: 17]   [loss avg: 3.0448]   [current loss: 0.0021]\n",
            "[Epoch: 18]   [loss avg: 2.8764]   [current loss: 0.0016]\n",
            "[Epoch: 19]   [loss avg: 2.7256]   [current loss: 0.0021]\n",
            "[Epoch: 20]   [loss avg: 2.5898]   [current loss: 0.0014]\n",
            "[Epoch: 21]   [loss avg: 2.4670]   [current loss: 0.0021]\n",
            "[Epoch: 22]   [loss avg: 2.3553]   [current loss: 0.0011]\n",
            "[Epoch: 23]   [loss avg: 2.2533]   [current loss: 0.0014]\n",
            "[Epoch: 24]   [loss avg: 2.1597]   [current loss: 0.0007]\n",
            "[Epoch: 25]   [loss avg: 2.0737]   [current loss: 0.0006]\n",
            "[Epoch: 26]   [loss avg: 1.9941]   [current loss: 0.0008]\n",
            "[Epoch: 27]   [loss avg: 1.9205]   [current loss: 0.0007]\n",
            "[Epoch: 28]   [loss avg: 1.8522]   [current loss: 0.0005]\n",
            "[Epoch: 29]   [loss avg: 1.7885]   [current loss: 0.0006]\n",
            "[Epoch: 30]   [loss avg: 1.7290]   [current loss: 0.0006]\n",
            "[Epoch: 31]   [loss avg: 1.6734]   [current loss: 0.0004]\n",
            "[Epoch: 32]   [loss avg: 1.6212]   [current loss: 0.0005]\n",
            "[Epoch: 33]   [loss avg: 1.5723]   [current loss: 0.0005]\n",
            "[Epoch: 34]   [loss avg: 1.5261]   [current loss: 0.0006]\n",
            "[Epoch: 35]   [loss avg: 1.4826]   [current loss: 0.0005]\n",
            "[Epoch: 36]   [loss avg: 1.4416]   [current loss: 0.0005]\n",
            "[Epoch: 37]   [loss avg: 1.4027]   [current loss: 0.0010]\n",
            "[Epoch: 38]   [loss avg: 1.3659]   [current loss: 0.0007]\n",
            "[Epoch: 39]   [loss avg: 1.3309]   [current loss: 0.0003]\n",
            "[Epoch: 40]   [loss avg: 1.2977]   [current loss: 0.0002]\n",
            "[Epoch: 41]   [loss avg: 1.2661]   [current loss: 0.0002]\n",
            "[Epoch: 42]   [loss avg: 1.2360]   [current loss: 0.0003]\n",
            "[Epoch: 43]   [loss avg: 1.2073]   [current loss: 0.0001]\n",
            "[Epoch: 44]   [loss avg: 1.1799]   [current loss: 0.0002]\n",
            "[Epoch: 45]   [loss avg: 1.1537]   [current loss: 0.0002]\n",
            "[Epoch: 46]   [loss avg: 1.1287]   [current loss: 0.0002]\n",
            "[Epoch: 47]   [loss avg: 1.1047]   [current loss: 0.0004]\n",
            "[Epoch: 48]   [loss avg: 1.0817]   [current loss: 0.0002]\n",
            "[Epoch: 49]   [loss avg: 1.0597]   [current loss: 0.0002]\n",
            "[Epoch: 50]   [loss avg: 1.0385]   [current loss: 0.0002]\n",
            "[Epoch: 51]   [loss avg: 1.0181]   [current loss: 0.0001]\n",
            "[Epoch: 52]   [loss avg: 0.9986]   [current loss: 0.0001]\n",
            "[Epoch: 53]   [loss avg: 0.9798]   [current loss: 0.0001]\n",
            "[Epoch: 54]   [loss avg: 0.9617]   [current loss: 0.0002]\n",
            "[Epoch: 55]   [loss avg: 0.9442]   [current loss: 0.0001]\n",
            "[Epoch: 56]   [loss avg: 0.9273]   [current loss: 0.0001]\n",
            "[Epoch: 57]   [loss avg: 0.9111]   [current loss: 0.0001]\n",
            "[Epoch: 58]   [loss avg: 0.8954]   [current loss: 0.0001]\n",
            "[Epoch: 59]   [loss avg: 0.8802]   [current loss: 0.0001]\n",
            "[Epoch: 60]   [loss avg: 0.8656]   [current loss: 0.0001]\n",
            "[Epoch: 61]   [loss avg: 0.8514]   [current loss: 0.0002]\n",
            "[Epoch: 62]   [loss avg: 0.8377]   [current loss: 0.0001]\n",
            "[Epoch: 63]   [loss avg: 0.8244]   [current loss: 0.0001]\n",
            "[Epoch: 64]   [loss avg: 0.8115]   [current loss: 0.0001]\n",
            "[Epoch: 65]   [loss avg: 0.7991]   [current loss: 0.0001]\n",
            "[Epoch: 66]   [loss avg: 0.7870]   [current loss: 0.0001]\n",
            "[Epoch: 67]   [loss avg: 0.7752]   [current loss: 0.0001]\n",
            "[Epoch: 68]   [loss avg: 0.7638]   [current loss: 0.0000]\n",
            "[Epoch: 69]   [loss avg: 0.7528]   [current loss: 0.0000]\n",
            "[Epoch: 70]   [loss avg: 0.7420]   [current loss: 0.0001]\n",
            "[Epoch: 71]   [loss avg: 0.7316]   [current loss: 0.0001]\n",
            "[Epoch: 72]   [loss avg: 0.7214]   [current loss: 0.0000]\n",
            "[Epoch: 73]   [loss avg: 0.7116]   [current loss: 0.0000]\n",
            "[Epoch: 74]   [loss avg: 0.7019]   [current loss: 0.0000]\n",
            "[Epoch: 75]   [loss avg: 0.6926]   [current loss: 0.0000]\n",
            "[Epoch: 76]   [loss avg: 0.6835]   [current loss: 0.0000]\n",
            "[Epoch: 77]   [loss avg: 0.6746]   [current loss: 0.0002]\n",
            "[Epoch: 78]   [loss avg: 0.6660]   [current loss: 0.0000]\n",
            "[Epoch: 79]   [loss avg: 0.6576]   [current loss: 0.0000]\n",
            "[Epoch: 80]   [loss avg: 0.6493]   [current loss: 0.0000]\n",
            "[Epoch: 81]   [loss avg: 0.6413]   [current loss: 0.0000]\n",
            "[Epoch: 82]   [loss avg: 0.6335]   [current loss: 0.0001]\n",
            "[Epoch: 83]   [loss avg: 0.6259]   [current loss: 0.0000]\n",
            "[Epoch: 84]   [loss avg: 0.6184]   [current loss: 0.0000]\n",
            "[Epoch: 85]   [loss avg: 0.6112]   [current loss: 0.0001]\n",
            "[Epoch: 86]   [loss avg: 0.6041]   [current loss: 0.0000]\n",
            "[Epoch: 87]   [loss avg: 0.5971]   [current loss: 0.0000]\n",
            "[Epoch: 88]   [loss avg: 0.5903]   [current loss: 0.0000]\n",
            "[Epoch: 89]   [loss avg: 0.5837]   [current loss: 0.0000]\n",
            "[Epoch: 90]   [loss avg: 0.5772]   [current loss: 0.0000]\n",
            "[Epoch: 91]   [loss avg: 0.5709]   [current loss: 0.0000]\n",
            "[Epoch: 92]   [loss avg: 0.5647]   [current loss: 0.0000]\n",
            "[Epoch: 93]   [loss avg: 0.5586]   [current loss: 0.0000]\n",
            "[Epoch: 94]   [loss avg: 0.5527]   [current loss: 0.0001]\n",
            "[Epoch: 95]   [loss avg: 0.5469]   [current loss: 0.0000]\n",
            "[Epoch: 96]   [loss avg: 0.5412]   [current loss: 0.0000]\n",
            "[Epoch: 97]   [loss avg: 0.5356]   [current loss: 0.0000]\n",
            "[Epoch: 98]   [loss avg: 0.5301]   [current loss: 0.0000]\n",
            "[Epoch: 99]   [loss avg: 0.5248]   [current loss: 0.0000]\n",
            "[Epoch: 100]   [loss avg: 0.5195]   [current loss: 0.0000]\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk8iF7iyvNpD",
        "colab_type": "text"
      },
      "source": [
        "## 模型测试"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mpq25osvQjs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "aec80168-6fe4-4489-c4c4-c904f228f276"
      },
      "source": [
        "count = 0\n",
        "# 模型测试\n",
        "\n",
        "net.eval()\n",
        "for inputs, _ in test_loader:\n",
        "    inputs = inputs.to(device)\n",
        "    outputs = net(inputs)\n",
        "    outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
        "    if count == 0:\n",
        "        y_pred_test =  outputs\n",
        "        count = 1\n",
        "    else:\n",
        "        y_pred_test = np.concatenate( (y_pred_test, outputs) )\n",
        "\n",
        "# 生成分类报告\n",
        "classification = classification_report(ytest, y_pred_test, digits=4)\n",
        "print(classification)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9535    1.0000    0.9762        41\n",
            "         1.0     0.9959    0.9541    0.9746      1285\n",
            "         2.0     0.9920    1.0000    0.9960       747\n",
            "         3.0     1.0000    0.9812    0.9905       213\n",
            "         4.0     0.9977    0.9977    0.9977       435\n",
            "         5.0     0.9894    0.9909    0.9901       657\n",
            "         6.0     1.0000    1.0000    1.0000        25\n",
            "         7.0     1.0000    1.0000    1.0000       430\n",
            "         8.0     1.0000    0.8889    0.9412        18\n",
            "         9.0     0.9920    0.9943    0.9932       875\n",
            "        10.0     0.9774    0.9973    0.9872      2210\n",
            "        11.0     0.9888    0.9906    0.9897       534\n",
            "        12.0     1.0000    1.0000    1.0000       185\n",
            "        13.0     0.9939    1.0000    0.9969      1139\n",
            "        14.0     1.0000    1.0000    1.0000       347\n",
            "        15.0     0.9750    0.9286    0.9512        84\n",
            "\n",
            "    accuracy                         0.9898      9225\n",
            "   macro avg     0.9910    0.9827    0.9865      9225\n",
            "weighted avg     0.9899    0.9898    0.9898      9225\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMrOWFr2vRUO",
        "colab_type": "text"
      },
      "source": [
        "## 备用函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiN2OOPFvUbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from operator import truediv\n",
        "\n",
        "def AA_andEachClassAccuracy(confusion_matrix):\n",
        "    counter = confusion_matrix.shape[0]\n",
        "    list_diag = np.diag(confusion_matrix)\n",
        "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
        "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_acc)\n",
        "    return each_acc, average_acc\n",
        "\n",
        "\n",
        "def reports (test_loader, y_test, name):\n",
        "    count = 0\n",
        "    # 模型测试\n",
        "    for inputs, _ in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = net(inputs)\n",
        "        outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
        "        if count == 0:\n",
        "            y_pred =  outputs\n",
        "            count = 1\n",
        "        else:\n",
        "            y_pred = np.concatenate( (y_pred, outputs) )\n",
        "\n",
        "    if name == 'IP':\n",
        "        target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
        "                        ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed', \n",
        "                        'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
        "                        'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n",
        "                        'Stone-Steel-Towers']\n",
        "    elif name == 'SA':\n",
        "        target_names = ['Brocoli_green_weeds_1','Brocoli_green_weeds_2','Fallow','Fallow_rough_plow','Fallow_smooth',\n",
        "                        'Stubble','Celery','Grapes_untrained','Soil_vinyard_develop','Corn_senesced_green_weeds',\n",
        "                        'Lettuce_romaine_4wk','Lettuce_romaine_5wk','Lettuce_romaine_6wk','Lettuce_romaine_7wk',\n",
        "                        'Vinyard_untrained','Vinyard_vertical_trellis']\n",
        "    elif name == 'PU':\n",
        "        target_names = ['Asphalt','Meadows','Gravel','Trees', 'Painted metal sheets','Bare Soil','Bitumen',\n",
        "                        'Self-Blocking Bricks','Shadows']\n",
        "    \n",
        "    classification = classification_report(y_test, y_pred, target_names=target_names)\n",
        "    oa = accuracy_score(y_test, y_pred)\n",
        "    confusion = confusion_matrix(y_test, y_pred)\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "    \n",
        "    return classification, confusion, oa*100, each_acc*100, aa*100, kappa*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TgsA6aNvWb2",
        "colab_type": "text"
      },
      "source": [
        "检测结果写在文件里："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL1cSNGJvYrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classification, confusion, oa, each_acc, aa, kappa = reports(test_loader, ytest, 'IP')\n",
        "classification = str(classification)\n",
        "confusion = str(confusion)\n",
        "file_name = \"classification_report.txt\"\n",
        "\n",
        "with open(file_name, 'w') as x_file:\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Kappa accuracy (%)'.format(kappa))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Overall accuracy (%)'.format(oa))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Average accuracy (%)'.format(aa))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{}'.format(classification))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{}'.format(confusion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-maQetKvaTW",
        "colab_type": "text"
      },
      "source": [
        "下面代码用于显示分类结果："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_UBFtGlvbrp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "f231841e-13c3-42ac-f920-b1f6c6f7890a"
      },
      "source": [
        "# load the original image\n",
        "X = sio.loadmat('Indian_pines_corrected.mat')['indian_pines_corrected']\n",
        "y = sio.loadmat('Indian_pines_gt.mat')['indian_pines_gt']\n",
        "\n",
        "height = y.shape[0]\n",
        "width = y.shape[1]\n",
        "\n",
        "X = applyPCA(X, numComponents= pca_components)\n",
        "X = padWithZeros(X, patch_size//2)\n",
        "\n",
        "# 逐像素预测类别\n",
        "outputs = np.zeros((height,width))\n",
        "for i in range(height):\n",
        "    for j in range(width):\n",
        "        if int(y[i,j]) == 0:\n",
        "            continue\n",
        "        else :\n",
        "            image_patch = X[i:i+patch_size, j:j+patch_size, :]\n",
        "            image_patch = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2], 1)\n",
        "            X_test_image = torch.FloatTensor(image_patch.transpose(0, 4, 3, 1, 2)).to(device)                                   \n",
        "            prediction = net(X_test_image)\n",
        "            prediction = np.argmax(prediction.detach().cpu().numpy(), axis=1)\n",
        "            outputs[i][j] = prediction+1\n",
        "    if i % 20 == 0:\n",
        "        print('... ... row ', i, ' handling ... ...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "... ... row  0  handling ... ...\n",
            "... ... row  20  handling ... ...\n",
            "... ... row  40  handling ... ...\n",
            "... ... row  60  handling ... ...\n",
            "... ... row  80  handling ... ...\n",
            "... ... row  100  handling ... ...\n",
            "... ... row  120  handling ... ...\n",
            "... ... row  140  handling ... ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JBqjG-dvdz3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "81232ec6-aa85-44d1-f631-56f8d7d7d21c"
      },
      "source": [
        "predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(5,5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}